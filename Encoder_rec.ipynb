{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=5)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=5)\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=5)\n",
    "        self.pool4 = nn.MaxPool2d(2)\n",
    "        self.conv5 = nn.Conv2d(256, 512, kernel_size=5)\n",
    "        self.pool5 = nn.MaxPool2d(2)\n",
    "        self.avgpool = nn.AvgPool2d(3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        x = self.pool3(x)\n",
    "        x = torch.relu(self.conv4(x))\n",
    "        x = self.pool4(x)\n",
    "        x = torch.relu(self.conv5(x))\n",
    "        x = self.pool5(x)\n",
    "        x = self.avgpool(x)\n",
    "        return x.view(x.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTMCell(embedding_dim + 512, hidden_dim)\n",
    "        self.output_layer = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, h_c):\n",
    "        embed = self.embedding(x)\n",
    "        input_lstm = torch.cat((embed, h_c[0]), dim=1)\n",
    "        h_c = self.lstm(input_lstm, h_c)\n",
    "        output = self.output_layer(h_c[0])\n",
    "        return output, h_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from nltk import word_tokenize\n",
    "from collections import Counter\n",
    "from nltk.util import ngrams\n",
    "\n",
    "class BLEU(object):\n",
    "    @staticmethod\n",
    "    def compute(candidate, references, weights):\n",
    "        candidate = [c.lower() for c in candidate]\n",
    "        references = [[r.lower() for r in reference] for reference in references]\n",
    "\n",
    "        p_ns = (BLEU.modified_precision(candidate, references, i) for i, _ in enumerate(weights, start=1))\n",
    "        s = math.fsum(w * math.log(p_n) for w, p_n in zip(weights, p_ns) if p_n)\n",
    "\n",
    "        bp = BLEU.brevity_penalty(candidate, references)\n",
    "        return bp * math.exp(s)\n",
    "    @staticmethod\n",
    "    def modified_precision(candidate, references, n):\n",
    "        counts = Counter(ngrams(candidate, n))\n",
    "\n",
    "        if not counts:\n",
    "            return 0\n",
    "\n",
    "        max_counts = {}\n",
    "        for reference in references:\n",
    "            reference_counts = Counter(ngrams(reference, n))\n",
    "            for ngram in counts:\n",
    "                max_counts[ngram] = max(max_counts.get(ngram, 0), reference_counts[ngram])\n",
    "\n",
    "        clipped_counts = dict((ngram, min(count, max_counts[ngram])) for ngram, count in counts.items())\n",
    "\n",
    "        return sum(clipped_counts.values()) / sum(counts.values())\n",
    "    \n",
    "    @staticmethod\n",
    "    def brevity_penalty(candidate, references):\n",
    "        c = len(candidate)\n",
    "\n",
    "        r = min(len(r) for r in references)\n",
    "\n",
    "        if c > r:\n",
    "            return 1\n",
    "        else:\n",
    "            return math.exp(1 - r / c)\n",
    "#jhbfrvjkndgbjk\n",
    "print(\"hello\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size= 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro Bleu :  0.6723318725973813\n"
     ]
    }
   ],
   "source": [
    "scorer = BLEU()\n",
    "grount_truths = [\"$ \\sin ^ { 2 } \\theta + \\cos ^ { 2 } \\theta = 1 $\",\n",
    "                \"$ \\sum _ { { T \\geq g } } { 8 . 2 } $\",\n",
    "                \"$ r = r ( \\theta ) $\"]\n",
    "\n",
    "\n",
    "# the predictions must be in the same format where each symbol is followed by a space\n",
    "predictions = [\"$ \\cos ^ { 2 } \\theta + \\cos ^ { 2 } \\theta = 1 } } } $  \",\n",
    "                \"$ \\sum _ { { T \\leq g } } { 0 . 2 } $\",\n",
    "                \"$ x = R ( \\theta ) $\"]\n",
    "\n",
    "\n",
    "overall = 0\n",
    "for gt, pred in zip(grount_truths, predictions):\n",
    "    gt = gt.split()\n",
    "    pred = pred.split()\n",
    "    overall += BLEU.compute(pred,[gt], weights=[1/4, 1/4, 1/4, 1/4])\n",
    "\n",
    "print(\"Macro Bleu : \", overall/len(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'handwritten_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\HARSH GARG\\Desktop\\Resnet_4\\Encoder_rec.ipynb Cell 6\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/HARSH%20GARG/Desktop/Resnet_4/Encoder_rec.ipynb#W5sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m start_token \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m<start>\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/HARSH%20GARG/Desktop/Resnet_4/Encoder_rec.ipynb#W5sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m end_token \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m<end>\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/HARSH%20GARG/Desktop/Resnet_4/Encoder_rec.ipynb#W5sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m handwritten_dataset \u001b[39m=\u001b[39m MathExpressionDataset(handwritten_data, start_token, end_token, transform\u001b[39m=\u001b[39mtransform)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/HARSH%20GARG/Desktop/Resnet_4/Encoder_rec.ipynb#W5sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m synthetic_dataset \u001b[39m=\u001b[39m MathExpressionDataset(synthetic_data, start_token, end_token, transform\u001b[39m=\u001b[39mtransform)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/HARSH%20GARG/Desktop/Resnet_4/Encoder_rec.ipynb#W5sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m batch_size\u001b[39m=\u001b[39m \u001b[39m32\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'handwritten_data' is not defined"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Assuming you have a custom dataset class\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Assuming you have a custom dataset class\n",
    "class MathExpressionDataset(Dataset):\n",
    "    def __init__(self, data, start_token, end_token, transform=None):\n",
    "        self.data = data  # Your dataset should contain pairs of images and corresponding LaTeX formulas\n",
    "        self.start_token = start_token\n",
    "        self.end_token = end_token\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path, latex_formula = self.data[index]\n",
    "        \n",
    "        # Load the image\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        # Apply transformations if needed\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        # Add start and end tokens to the LaTeX formula\n",
    "        latex_formula = f\"{self.start_token} {latex_formula} {self.end_token}\"\n",
    "        \n",
    "        # Return image and corresponding LaTeX formula\n",
    "        return img, latex_formula\n",
    "\n",
    "# Example transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Create datasets with start and end tokens\n",
    "start_token = \"<start>\"\n",
    "end_token = \"<end>\"\n",
    "handwritten_dataset = MathExpressionDataset(handwritten_data, start_token, end_token, transform=transform)\n",
    "synthetic_dataset = MathExpressionDataset(synthetic_data, start_token, end_token, transform=transform)\n",
    "batch_size= 32\n",
    "# Create dataloaders\n",
    "handwritten_dataloader = DataLoader(handwritten_dataset, batch_size=batch_size, shuffle=True)\n",
    "synthetic_dataloader = DataLoader(synthetic_dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import numpy as np\n",
    "\n",
    "# Assuming you have defined Encoder and Decoder classes as mentioned before\n",
    "\n",
    "# Define the dataset and dataloaders\n",
    "# ...\n",
    "\n",
    "# Hyperparameters\n",
    "\n",
    "embedding_dim = 512\n",
    "hidden_dim = 512\n",
    "learning_rate = 0.001\n",
    "epochs = 10\n",
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "# Instantiate the encoder and decoder\n",
    "encoder = Encoder()\n",
    "decoder = Decoder(output_vocab_size, embedding_dim, hidden_dim)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    for input_data, target_data in dataloader:\n",
    "        # Forward pass\n",
    "        encoder_output = encoder(input_data)\n",
    "        \n",
    "        # Initialize decoder hidden state and cell state\n",
    "        h_c = (torch.zeros(batch_size, hidden_dim), torch.zeros(batch_size, hidden_dim))\n",
    "        \n",
    "        # Initialize input for the first timestep\n",
    "        input_seq = torch.tensor([START_TOKEN] * batch_size)  # START_TOKEN is your token for the start of a sequence\n",
    "        \n",
    "        # Use teacher forcing with a certain probability\n",
    "        use_teacher_forcing = np.random.random() < teacher_forcing_ratio\n",
    "        \n",
    "        # Training the decoder\n",
    "        for t in range(target_data.size(1)):\n",
    "            # Choose whether to use teacher forcing or not\n",
    "            if use_teacher_forcing and t > 0:\n",
    "                input_seq = target_data[:, t-1]\n",
    "            \n",
    "            # Embedding of the previous output\n",
    "            prev_output_embedding = decoder.embedding(input_seq)\n",
    "            \n",
    "            # Concatenate context vector with the embedding\n",
    "            lstm_input = torch.cat((prev_output_embedding, encoder_output), dim=1)\n",
    "            \n",
    "            # Forward pass through the decoder\n",
    "            output, h_c = decoder(lstm_input, h_c)\n",
    "            \n",
    "            # Compute the loss\n",
    "            loss = criterion(output, target_data[:, t])\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "# Evaluation - BLEU score calculation\n",
    "# ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
